# Network_flow-and-IoT_telemetry
Smart-Home Cyber-Physical Dataset
The compiled corpus integrates two complementary time-series views of a small-scale smart-environment: (i) packet-level network-flow telemetry (4 893 rows) and (ii) device-side sensor readings (5 200 rows). Every record in both files is anchored to a one-second–granularity ISO-8601 timestamp that starts on 23 June 2024 18:00:00 UTC and advances monotonically, guaranteeing straightforward temporal alignment without resampling. Such clock coherence is critical for downstream tasks that fuse cyber- and physical-world traces—e.g., causality mining, cross-domain correlation, or latency-aware anomaly detection.
Structurally, the flow table captures the canonical 5-tuple plus protocol, packet size, flow-duration, and traffic direction, enriched with a categorical ground-truth label that distinguishes benign traffic from three common IoT attack archetypes (Mirai, DDoS floods, reconnaissance). This blend of continuous (size, duration) and categorical (protocol, direction, label) attributes makes the set suitable for a wide range of modelling paradigms—statistical baselines, classical ML, or neural architectures that ingest mixed data types. The presence of device identifiers and both private and public-facing destination IPs enables per-asset profiling and network graph construction, while the deliberate 90 / 10 benign-to-malicious ratio approximates real deployment skew and prevents trivial class-imbalance artefacts.
The sensor table offers a parallel physical context with four modalities—temperature, humidity, power-draw, and binary motion flags—each emitting values within empirically realistic ranges and units. An alert_flag column encodes domain-specific threshold breaches, injecting rare-event labels that are invaluable for imbalanced-scenario learning, streaming novelty detection, or cost-sensitive classification studies. Because each sensor entry references the same device namespace used in the flow log, researchers can test hypotheses that hinge on cyber-physical interplay, such as “motion-triggered camera streams” or “thermostat adjustments preceding traffic bursts.”
From a research methodology standpoint, the datasets are deliberately modest in scale (≈10 k rows) to facilitate rapid experimentation, yet rich enough to benchmark temporal models, representation learning techniques, and data-fusion pipelines. Their clean schema (no missing values, pre-validated units) allows immediate ingestion into common frameworks, while the documented attribute semantics encourage transparent feature engineering and reproducible baselines. Finally, because the data are synthetic but statistically constrained to match real-world distributions, they sidestep privacy concerns and can be freely redistributed or extended—serving as a convenient sandbox for prototyping before migration to proprietary or sensitive corpora.

